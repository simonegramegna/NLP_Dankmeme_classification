{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 695
    },
    "colab_type": "code",
    "id": "Pgg73iS_hvOh",
    "outputId": "8d7e3fc8-56c5-4ddd-bfed-7753208e0579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 112kB 2.8MB/s \n",
      "\u001b[K     |████████████████████████████████| 890kB 7.8MB/s \n",
      "\u001b[K     |████████████████████████████████| 71kB 6.8MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.2MB 9.5MB/s \n",
      "\u001b[K     |████████████████████████████████| 2.2MB 20.2MB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 7.9MB/s \n",
      "\u001b[K     |████████████████████████████████| 368kB 21.5MB/s \n",
      "\u001b[K     |████████████████████████████████| 829kB 31.5MB/s \n",
      "\u001b[K     |████████████████████████████████| 512kB 50.6MB/s \n",
      "\u001b[K     |████████████████████████████████| 8.6MB 44.1MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.4MB 48.3MB/s \n",
      "\u001b[K     |████████████████████████████████| 5.5MB 21.3MB/s \n",
      "\u001b[K     |████████████████████████████████| 890kB 49.2MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.1MB 47.6MB/s \n",
      "\u001b[K     |████████████████████████████████| 3.0MB 50.3MB/s \n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/1a/95/f50352b5366e7d579e8b99631680a9e32e1b22adfa1629a8f23b1d22d5e2/multidict-4.7.6-cp36-cp36m-manylinux1_x86_64.whl\u001b[0m\n",
      "\u001b[K     |████████████████████████████████| 153kB 2.8MB/s \n",
      "\u001b[K     |████████████████████████████████| 266kB 8.8MB/s \n",
      "\u001b[K     |████████████████████████████████| 51kB 6.0MB/s \n",
      "\u001b[K     |████████████████████████████████| 655kB 8.6MB/s \n",
      "\u001b[K     |████████████████████████████████| 92kB 6.2MB/s \n",
      "\u001b[K     |████████████████████████████████| 102kB 7.9MB/s \n",
      "\u001b[?25h  Building wheel for tornado (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for bokeh (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for locket (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: panel 0.9.7 has requirement bokeh>=2.1, but you'll have bokeh 2.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: nbclient 0.5.0 has requirement jupyter-client>=6.1.5, but you'll have jupyter-client 5.3.5 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=5.1.0; python_version >= \"3.0\", but you'll have tornado 5.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: distributed 2.26.0 has requirement cloudpickle>=1.5.0, but you'll have cloudpickle 1.3.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -q tensorflow numpy pandas scikit-learn mlxtend dataprep transformers\n",
    "! cp drive/My\\ Drive/Colab\\ Notebooks/*.csv ./\n",
    "! cp drive/My\\ Drive/Colab\\ Notebooks/*.pkl ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "alG5ImZ0C0VV"
   },
   "outputs": [],
   "source": [
    "def augment(training):\n",
    "  training = np.repeat(training, 10, axis=0)\n",
    "  temp = pickle.load(open('alexnet_embeddings.pkl', 'rb'))\n",
    "  for i in range(0, len(training), 10):\n",
    "    for j in range(len(X)):\n",
    "        if np.all(training[i][:1000] == X[j][:1000]):\n",
    "          for k in range(1, 10):\n",
    "            training[i+k] = np.concatenate([temp[j*10+k][:1000], training[i+k][1000:]])\n",
    "          break\n",
    "  return training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5VCOTGzyv1F"
   },
   "outputs": [],
   "source": [
    "def ds_from_df(df, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('Meme')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YWH9P4UkyuR-"
   },
   "outputs": [],
   "source": [
    "def create_map():\n",
    "  temp = df[['Visual']].values\n",
    "  counter = 0\n",
    "  map = {}\n",
    "  for i in temp:\n",
    "    for j in i[0].split():\n",
    "      if j.strip(',') not in map and j.strip(',') != '0':\n",
    "        map[j.strip(',')] = counter\n",
    "        counter += 1\n",
    "  return map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aj-N63Q-0rZv"
   },
   "outputs": [],
   "source": [
    "def get_X(temp):\n",
    "  res = []\n",
    "  for i in range(0, len(temp), 10):\n",
    "    res.append(temp[i])\n",
    "  return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "djLoSoayytuV"
   },
   "outputs": [],
   "source": [
    "def ohe(map):\n",
    "  one_hot_encode = []\n",
    "  temp = df[['Visual']].values\n",
    "\n",
    "  for i in temp:\n",
    "    arr = list(np.zeros(len(map),dtype=int))\n",
    "    for j in i[0].split():\n",
    "      if j.strip(',') != '0':\n",
    "        arr[map[j.strip(',')]] = 1\n",
    "    one_hot_encode.append(arr)\n",
    "  return np.array(one_hot_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zNKb12FA9zPW"
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape=(None, 2889)):\n",
    "  model = tf.keras.Sequential([tf.keras.layers.Dense(1024, \n",
    "                                                     name='dense_1',\n",
    "                                                     activation=tf.keras.activations.relu),\n",
    "                               tf.keras.layers.Dropout(rate=0.3),\n",
    "                               tf.keras.layers.Dense(512, \n",
    "                                                     name='dense_2',\n",
    "                                                     activation=tf.keras.activations.relu),\n",
    "                               tf.keras.layers.Dense(512, \n",
    "                                                     name='dense_3',\n",
    "                                                     activation=tf.keras.activations.relu),\n",
    "                               tf.keras.layers.Dense(128, \n",
    "                                                     name='dense_4',\n",
    "                                                     activation=tf.keras.activations.relu),\n",
    "                               tf.keras.layers.Dense(64, \n",
    "                                                     name='dense_5',\n",
    "                                                     activation=tf.keras.activations.relu),\n",
    "                               tf.keras.layers.Dense(1, name='output',\n",
    "                                                     activation=tf.keras.activations.sigmoid)])\n",
    "  print(model.compute_output_shape(input_shape=input_shape))\n",
    "  model.build(input_shape=input_shape)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1GaNrhEHlIwL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dankmemes_task1_train.csv')\n",
    "embedding = pd.read_csv('dankmemes_task1_train_embeddings.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qvVgi7MjyopT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "### Alternate these three, in order to dervie each feedforward network!\n",
    "\n",
    "# X, y = np.array([embedding[1][i].split() for i in range(1600)]).astype(float), df[['Meme']].values\n",
    "# X, y = get_X(pickle.load(open('alexnet_embeddings.pkl', 'rb'))), df[['Meme']].values\n",
    "# X, y = pickle.load(open('densenest_embeddings.pkl', 'rb')), df[['Meme']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OiKt6ID3yoKp"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "ssc = StandardScaler()\n",
    "mms = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "colab_type": "code",
    "id": "RbllPvbVykuv",
    "outputId": "05f5c0e7-0df8-4460-a81f-f8648bc74b5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 1)\n",
      "(1600, 1001)\n",
      "(1600, 1)\n",
      "(1600, 1002)\n",
      "(1600, 71)\n",
      "(1600, 1073)\n",
      "(1600, 768)\n",
      "(1600, 1841)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from datetime import date\n",
    "\n",
    "temp = mms.fit_transform(np.array([(date(int(i[0].split('-')[0]), int(i[0].split('-')[1]), int(i[0].split('-')[2])) - date(2015, 1, 1)).days for i in df[['Date']].values.tolist()]).reshape(1600, 1))\n",
    "print(temp.shape)\n",
    "X = np.hstack((X, temp))\n",
    "print(X.shape)\n",
    "\n",
    "temp = ssc.fit_transform(df[['Engagement']].values)\n",
    "print(temp.shape)\n",
    "X = np.hstack((X, temp))\n",
    "print(X.shape)\n",
    "\n",
    "temp = ohe(create_map())\n",
    "print(temp.shape)\n",
    "X = np.hstack((X, temp))\n",
    "print(X.shape)\n",
    "\n",
    "temp = pickle.load(open('cls_umberto_embeddings.pkl', 'rb'))\n",
    "print(temp.shape)\n",
    "X = np.hstack((X, temp))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m1eUkI3H-OeB"
   },
   "outputs": [],
   "source": [
    "buffer_size = 10000\n",
    "batch_size = 64\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "6VFHiDsEy6iQ",
    "outputId": "af46b0e9-619d-41d7-d6e0-992863d897bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 1841) (1280, 1841) (1280, 1) (320, 1841) (320, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "# X_train = augment(X_train)\n",
    "# y_train = np.repeat(y_train, 10, axis=0)\n",
    "print(X.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "ds_train = ds_train.shuffle(buffer_size=buffer_size,\n",
    "                            reshuffle_each_iteration=False)\n",
    "ds_test = ds_test.batch(batch_size)\n",
    "ds_train = ds_train.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "DngRJyN_-nHk",
    "outputId": "36245f25-ee04-4357-e4a3-1305b6d500e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1)\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1024)              1886208   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 2,747,649\n",
      "Trainable params: 2,747,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(input_shape=(None, 1841)) ## Modify input shape!\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "IPzpe4i_-sMB",
    "outputId": "ff3637bd-00a4-4610-deba-0fc5ddd2792b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 0.6210 - binary_accuracy: 0.6687 - val_loss: 0.4734 - val_binary_accuracy: 0.7688\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 0.5085 - binary_accuracy: 0.7469 - val_loss: 0.4534 - val_binary_accuracy: 0.7844\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 0.4725 - binary_accuracy: 0.7836 - val_loss: 0.4874 - val_binary_accuracy: 0.7563\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 0.4401 - binary_accuracy: 0.7969 - val_loss: 0.4410 - val_binary_accuracy: 0.7875\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 0.4085 - binary_accuracy: 0.8172 - val_loss: 0.4273 - val_binary_accuracy: 0.8188\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 0.3918 - binary_accuracy: 0.8281 - val_loss: 0.4097 - val_binary_accuracy: 0.8094\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 0.3647 - binary_accuracy: 0.8383 - val_loss: 0.4570 - val_binary_accuracy: 0.7812\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 0.3446 - binary_accuracy: 0.8500 - val_loss: 0.4626 - val_binary_accuracy: 0.8000\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 0.3319 - binary_accuracy: 0.8531 - val_loss: 0.4251 - val_binary_accuracy: 0.8094\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 0.2962 - binary_accuracy: 0.8680 - val_loss: 0.4622 - val_binary_accuracy: 0.8188\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(ds_train,\n",
    "                 validation_data=ds_test,\n",
    "                 epochs=num_epochs,\n",
    "                 batch_size=batch_size,\n",
    "                 use_multiprocessing=True,\n",
    "                 callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=5, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "eMk5qUgXSbmh",
    "outputId": "e7874aaf-c82a-427c-e6a3-ba05e6e9220e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/alexnet_ffwd/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('models/alexnet_ffwd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_5YYD5S-4JVo"
   },
   "outputs": [],
   "source": [
    "! cp -r models ./drive/My\\ Drive/Colab\\ Notebooks/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "augmented_feedforward.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
